{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sized-split",
   "metadata": {},
   "source": [
    "# 워드 임베딩\n",
    "\n",
    "### 벡터화\n",
    "\n",
    "- 기계가 자연어 처리를 원활히 할 수 있도록, 전처리 과정에서 텍스트를 벡터로 변환하는 과정\n",
    "\n",
    "#### 1) BoW / DTM(Document-Term Matrix, 문서 단어 행렬)\n",
    "- BoW는 단어의 순서를 고려하지 않고, 단어의 등장 빈도만을 고려해 단어를 벡터화하는 방법이다.\n",
    "- 한계\n",
    "    - (1) Sparsity\n",
    "    - (2) Frequent words has more power\n",
    "    - (3) Ignoring orders\n",
    "    - (4) OOV = 단어 사전에 없는 단어를 처리할 수 없는 문제\n",
    "\n",
    "#### 2) TF/IDF(Term Frequence * Inverse Document Frequency)\n",
    "\n",
    "- TF: 한 문서 내에서 등장 빈도에 따라 단어의 중요도를 score로 나타낸 것\n",
    "- 즉 문장을 구성하는 단어들의 원핫 벡터들을 모두 더해서 문장의 단어 개수로 나눈 것이다.\n",
    "- IDF: 의미는 중요하지 않은데 단지 모든 문서에서 자주 등장하는 단어의 중요도를 score로 계산한 것(log를 씌워서 모든 문서에서 해당 단어가 등장할수록 값이 작아짐)\n",
    "\n",
    "#### 3) 벡터화한 걸 어디에 쓰나?\n",
    "1. 유사도 계산 -> DTM(문서를 행, 단어를 열로 구성\n",
    "2. 머신러닝 모델의 입력 값으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-chamber",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 실습 1: 원핫 인코딩 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proprietary-settle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /opt/conda/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in /opt/conda/lib/python3.7/site-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.7/site-packages (from konlpy) (1.19.5)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from konlpy) (0.4.4)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /opt/conda/lib/python3.7/site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from konlpy) (1.2.1)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.7/site-packages (from konlpy) (4.6.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /opt/conda/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (2.25.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "necessary-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "casual-heating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'임금님 귀는 당나귀 귀! 임금님 귀는 당나귀 귀! 실컷~ 소리치고 나니 속이 확 뚫려 살 것 같았어.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"임금님 귀는 당나귀 귀! 임금님 귀는 당나귀 귀! 실컷~ 소리치고 나니 속이 확 뚫려 살 것 같았어.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prospective-formula",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임금님 귀는 당나귀 귀 임금님 귀는 당나귀 귀 실컷 소리치고 나니 속이 확 뚫려 살 것 같았어\n"
     ]
    }
   ],
   "source": [
    "# 전처리\n",
    "reg = re.compile(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\")\n",
    "text = reg.sub('', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "welsh-bermuda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['임금님', '귀', '는', '당나귀', '귀', '임금님', '귀', '는', '당나귀', '귀', '실컷', '소리', '치고', '나니', '속이', '확', '뚫려', '살', '것', '같았어']\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "okt=Okt()\n",
    "tokens = okt.morphs(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "happy-brunei",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'귀': 4, '임금님': 2, '는': 2, '당나귀': 2, '실컷': 1, '소리': 1, '치고': 1, '나니': 1, '속이': 1, '확': 1, '뚫려': 1, '살': 1, '것': 1, '같았어': 1})\n"
     ]
    }
   ],
   "source": [
    "# 단어장 만들기\n",
    "# 빈도수가 높은 순서대로 배치\n",
    "vocab = Counter(tokens)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "political-nevada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 등장 빈도 확인\n",
    "vocab['임금님']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sunset-installation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('귀', 4), ('임금님', 2), ('는', 2), ('당나귀', 2), ('실컷', 1)]\n"
     ]
    }
   ],
   "source": [
    "# most_common()은 상위 빈도수의 단어를 주어진 수만큼 리턴\n",
    "# 상위 빈도수 5개 단어 출력\n",
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dense-machine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'귀': 1, '임금님': 2, '는': 3, '당나귀': 4, '실컷': 5}\n"
     ]
    }
   ],
   "source": [
    "# 상위 5개 단어만으로 사전을 만들기\n",
    "word2idx={word[0] : index+1 for index, word in enumerate(vocab)}\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "global-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫벡터 만들기\n",
    "def one_hot_encoding(word, word2index):\n",
    "    one_hot_vector = [0]*(len(word2index))\n",
    "    index = word2index[word]\n",
    "    one_hot_vector[index-1] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "varied-beads",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"임금님\", word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "selected-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras로 원핫벡터 만들기\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "intermediate-diagnosis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['강아지', '고양이', '강아지'], ['애교', '고양이'], ['컴퓨터', '노트북']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [['강아지', '고양이', '강아지'],['애교', '고양이'], ['컴퓨터', '노트북']]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bearing-prophet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'강아지': 1, '고양이': 2, '애교': 3, '컴퓨터': 4, '노트북': 5}\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 만들기\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(text)\n",
    "print(t.word_index) # 각 단어에 대한 인코딩 결과 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "antique-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding 토큰을 고려해 1을 더해줌\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "universal-wagon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "sub_text = ['강아지', '고양이', '강아지', '컴퓨터']\n",
    "encoded = t.texts_to_sequences([sub_text])\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "false-treat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "one_hot = to_categorical(encoded, num_classes = vocab_size)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-climate",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 워드 임베딩\n",
    "\n",
    "### - 희소 벡터의 문제점\n",
    "\n",
    "- DTM, TF-IDF, 원핫벡터는 단어장의 크기게 영향을 받는 희소 벡터이다. 이들은 전체 차원 중 하나의 원소만 1이고 나머지는 모두 0인 값을 가진다.\n",
    "- 이러한 희소 벡터에는 차원의 저주 문제가 있다. 정보 밀도가 작아지는 것, 즉 차원이 커지는 것과 머신러닝 모델의 성능에는 어떤 연관 관계가 있다.\n",
    "- 데이터에서 모델을 학습할 때 독립적 샘플이 많을수록 학습이 잘 되는 반면 차원이 커질수록 학습이 어려워지고 더 많은 데이터를 필요로 한다. 이는 선형 회귀 문제에서 feature가 많을 때 모델의 일반화 성능이 잘 나오지 않는 것과도 같은 맥락이다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/135985433-e3aa6f26-8871-4b5d-81e3-91fdf71c3786.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/135986177-825f3ea6-6d84-4d16-b316-d26123ecb64d.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/135986350-5dd9d3d8-b166-4ede-a41e-3cb923b5b2e6.png)\n",
    "\n",
    "- 또 다른 문제는 의미상 유사도를 계산할 수 없다는 것이다.\n",
    "- 따라서 그 대안으로 워드 임베딩이 제안되었다.\n",
    "\n",
    "### - 워드 임베딩\n",
    "\n",
    "- 워드 임베딩은 기계가 단어장 크기보다 적은 차원의 밀집 벡터를 학습하는 방식이다. 이를 통해 얻는 밀집 벡터는 각 차원이 0과 1이 아닌 다양한 실수 값을 가진다.\n",
    "- 밀집 표현(Dense Representation)은 벡터의 차원을 단어 집합의 크기로 상정하지 않고, 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춘다.\n",
    "- 워드 임베딩 방법론으로는 LSA, Word2Vec, FastText, Glove 등이 있다.\n",
    "- keras의 Embedding()에서는 위 방법들을 사용하지는 않지만, 단어를 랜덤한 값을 가지는 밀집 벡터로 변환한 뒤에, 인공 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-quebec",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 워드 임베딩 1: Word2Vec\n",
    "\n",
    "- 학습된 한국어 Word2Vec 벡터들로 연산한 결과를 제공하는 [사이트](https://word2vec.kr/search/)\n",
    "- 예를 들어, 박찬호 - 야구 + 축구 = 호나우두\n",
    "- 함께 사용되는 단어들(neighbor)일수록 유사도가 높다고 판단\n",
    "- 크게 CBoW와 skip-gram의 두 가지 방법이 있다.\n",
    "- CBoW는 주변에 있는 단어들을 통해 중간 단어를 예측하는 방법이고, skipgram은 반대로 중간에 있는 단어로 주변 단어들을 예측하는 방법이다.\n",
    "- skipgram에서 window size=1일 경우, 해당 단어 옆의 1개 단어만 살펴보겠다는 의미\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136125368-07bc7fa0-42e5-4c7d-9003-417fea5872cf.png)\n",
    "\n",
    "- 아래 그림에서 hidden layer가 바로 word2vec이 된다.\n",
    "- output에 softmax를 취하므로 모두 합한 값은 1이 된다(확률).\n",
    "- 마지막으로 output과 target의 차이로 cross-entropy를 계산해서 back-propagation을 해서 가중치를 업데이트\n",
    "- 이 가중치가 embedding 값이다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136125634-6b6f69db-17ba-4785-94db-c9588c408582.png)\n",
    "\n",
    "- input은 그냥 단어를 원핫인코딩한 벡터\n",
    "- 우리가 구하고 싶은 embedding은 hidden layer, 즉 w 가중치\n",
    "- 결국 이 embedding 벡터를 가지고 유사도를 따질 수 있다. 그리고 심지어 그래프에 plot하면 물리적 거리가 당연히 가깝게 나옴\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136125912-a5a01145-a4e4-4815-88da-c98772bafdd9.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136125992-6f65049d-bffd-4b5e-86d4-a47600ee60f2.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136126021-556f95fe-e649-4970-9d69-18c265a5bba5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-startup",
   "metadata": {},
   "source": [
    "#### - 분포 가설(Distributional Hypothesis)\n",
    "\n",
    "- 위와 같은 Word2Vec의 방식은 바로 분포 가설을 따른다. 즉 어떤 단어들의 의미를 보려면 주변 단어들을 보면 알 수 있다는 것이다. 비슷한 문맥에서 같이 등장하는 경향이 있는 단어들은 비슷한 의미를 가진다는 것이다.\n",
    "\n",
    "### Word2Vec (1) CBoW(Continous Bag of Words)\n",
    "\n",
    "- 주변 단어(context word)들로 중심 단어(center word)를 예측하는 방법\n",
    "- 중심 단어를 예측하기 위해 앞, 뒤로 몇 개의 단어를 볼지 그 범위를 윈도우(window)라고 한다.\n",
    "- 윈도우 크기를 정하면, 이 윈도우를 움직이면서 주변 단어와 중심 단어를 바꿔가며 학습을 위한 데이터 셋을 만들 수 있다. 이를 슬라이딩 윈도우라고 한다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136127281-1865ae7c-b501-448b-840a-82c39947ad86.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136128473-2c432ee8-978c-4c97-add0-ad51c4f54e90.png)\n",
    "\n",
    "- 위 그림은 원핫 벡터로 변환된 다수의 주변 단어를 이용해 원핫 벡터로 변환된 중심 단어를 예측할 때의 CBoW의 동작 메커니즘을 보여준다.\n",
    "- 윈도우 크기가 m이라면 2m개의 주변 단어를 이용해 1개의 중심 단어를 예측하는 과정에서 두 개의 가중치 행렬을 학습하는 것이 목적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-yorkshire",
   "metadata": {},
   "source": [
    "- 주황색 행렬이 첫 번째 가중치 행렬 W, 초록색이 두 번째 가중치 행렬 W'이 된다.\n",
    "- 이는 인공 신경망 구조상 입력층, 은닉층, 출력층의 3개 층으로 구성된 것으로 볼 수 있다.\n",
    "- 입력층과 출력층의 크기는 단어 집합의 크기인 V로 고정된다.\n",
    "- 그러나 은닉층의 크기는 사용자가 지정하는 하이퍼파라미터이며, N, 즉 word embedding dimension을 의미한다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136139077-c5f3a488-9aca-45f9-8272-e6e627764d4f.png)\n",
    "\n",
    "- 주변 단어로 선택된 각각의 원핫 벡터는 첫 번째 가중치 행렬과 곱해지게 되고, 이 가중치 행렬의 크기는 V x N이다.\n",
    "- 이때 원핫 벡터는 각 단어의 해당하는 인덱스 i에서만 1의 값을 가지므로, 가중치 행렬과의 곱은 행렬의 i번째 row를 그대로 가져오는 것과 동일하다.\n",
    "- 이처럼 마치 테이블에서 값을 그대로 룩업해오는 것과 같다고 하여 lookup table이라고 부른다.\n",
    "- 룩업 테이블을 거쳐 얻은 2m개의 주변 단어 벡터들은 각각 N의 크기를 가진다. CBoW에서는 이 벡터들을 모두 합한 값이나 평균 값을 은닉층의 최종 결과로 도출한다.\n",
    "- Word2Vec에서는 은닉층에서 활성화 함수나 편향을 더하는 연산은 하지 않는다. 때문에 다른 신경망의 은닉층과 구분지어 투사층(Projection layer)라고도 한다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136141829-d27c2e7a-b2c0-4fb5-8a59-ad317ef1337a.png)\n",
    "\n",
    "- 은닉층에서 생성된 N차원 벡터는 두 번째 가중치 행렬과 곱해진다. 이 행렬의 크기는 N x V이고, 따라서 곱셈의 결과로 V 차원의 벡터를 얻을 수 있다.\n",
    "- 출력층은 활성화 함수로 소프트맥스 함수를 사용하므로 이 V차원의 벡터는 결국 모든 차원의 총합이 1이 되는 벡터로 변환된다.\n",
    "- 이렇게 얻은 출력층의 벡터를 중심 단어의 원핫 벡터와 비고해 그 손실이 최소화되도록 학습한다.\n",
    "- 학습이 완료되면 N차원 크기를 갖는 W의 row나 W'의 column 중 어떤 것을 임베딩 벡터로 사용할지 결정한다. 또는 둘의 평균 값을 선택하기도 한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "supposed-salon",
   "metadata": {},
   "source": [
    "### Word2Vec (2) Skip-gramp과 Negative Sampling\n",
    "\n",
    "- skipgram은 CBoW와 반대로 중심 단어로 주변 단어를 예측한다. 따라서 데이터셋의 형식은 (중심 단어, 주변 단어)로 다음과 같다.\n",
    "- (i, like) (like, I), (like, natural), (natural, like), (natural, language), (language, natural), (language, processing), (processing, language)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136142187-d5abd32f-d9da-4cfe-b369-ad23fd7ce2dc.png)\n",
    "\n",
    "- CBoW의 메커니즘과 다른 점은 중심 단어로부터 주변 단어를 예측하는 점과 은닉층에서 벡터의 덧셈과 평균 계산 과정이 없다는 것이다.\n",
    "- 또 CBoW와 마찬가지로 학습 후에 가중치 행렬 W의 row 또는 W'의 column으로부터 임베딩 벡터를 얻을 수 있다.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 대체로 Word2Vec을 사용할 때는 SGNS(Skip-Gram with Negative Sampling)을 사용한다. 즉 skipgram을 사용하면서 네거티브 샘플링도 사용하는 것. 왜냐하면 word2vec의 구조는 연산량이 지나치게 많아서 실제로 사용하기 어렵다!\n",
    "- 예를 들어 중심 단어와 주변 단어가 사과, 딸기와 같은 과일이라면, 필통이나 연필 같이 연관 관계가 없는 단어들의 임베딩 값을 매번 업데이트할 필요가 없다. 따라서 네거티브 샘플링은 연산량을 줄이기 위해 소프트맥스 함수를 사용한 V개 중 1개를 고르는 다중 클래스 분류 문제를 시그모이드 함수를 사용한 이진 분류 문제로 바꾸는 것이다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136146463-d2ffc51d-369b-4aa9-a0c1-566934f2f93f.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136146484-fd9c53df-4877-4d6a-8a04-5712be47c427.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136146502-192e3fcd-0185-4cce-bdc1-05efec7a2a4a.png)\n",
    "\n",
    "- target word의 경우 target label을 1로, 다른 단어들은 0으로 labeling해주는 것! 이때 거짓(negative) 데이터셋을 만들기 때문에 이를 네거티브 샘플링이라고 한다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136147026-8cd9f62f-7ef9-449c-9322-84501b0d2a42.png)\n",
    "\n",
    "- 이렇게 완성된 데이터셋으로 학습하면 이진 분류 문제로 간주할 수 있다. 중심 단어와 주변 단어를 내적하고, 출력층의 시그모이드 함수를 지나게 하여 1 또는 0의 레이블로부터 오차를 구해서 역전파를 수행한다.\n",
    "- 이렇게 하면 기존 소프트맥스 함수 방식보다 연산량이 획기적으로 줄어든다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-staff",
   "metadata": {},
   "source": [
    "### Word2Vec (3) 실습\n",
    "\n",
    "- gensim 패키지 이용해서 토픽 모델링, 훈련 데이터는 NLTK에서 제공하는 corpus 이용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "attractive-confusion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.5)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.56.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.7/site-packages (4.0.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legal-hawaii",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package abc to /aiffel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/abc.zip.\n",
      "[nltk_data] Downloading package punkt to /aiffel/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('abc')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "numeric-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus 불러오기\n",
    "from nltk.corpus import abc\n",
    "\n",
    "corpus = abc.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "delayed-commerce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PM', 'denies', 'knowledge', 'of', 'AWB', 'kickbacks', 'The', 'Prime', 'Minister', 'has', 'denied', 'he', 'knew', 'AWB', 'was', 'paying', 'kickbacks', 'to', 'Iraq', 'despite', 'writing', 'to', 'the', 'wheat', 'exporter', 'asking', 'to', 'be', 'kept', 'fully', 'informed', 'on', 'Iraq', 'wheat', 'sales', '.'], ['Letters', 'from', 'John', 'Howard', 'and', 'Deputy', 'Prime', 'Minister', 'Mark', 'Vaile', 'to', 'AWB', 'have', 'been', 'released', 'by', 'the', 'Cole', 'inquiry', 'into', 'the', 'oil', 'for', 'food', 'program', '.'], ['In', 'one', 'of', 'the', 'letters', 'Mr', 'Howard', 'asks', 'AWB', 'managing', 'director', 'Andrew', 'Lindberg', 'to', 'remain', 'in', 'close', 'contact', 'with', 'the', 'Government', 'on', 'Iraq', 'wheat', 'sales', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "victorian-hormone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코퍼스의 크기 : 29059\n"
     ]
    }
   ],
   "source": [
    "print('코퍼스의 크기 :',len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "twenty-listing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-ultimate",
   "metadata": {},
   "source": [
    "- vector size = 학습 후 임베딩 벡터의 차원\n",
    "- window = 컨텍스트 윈도우 크기\n",
    "- min_count = 단어 최소 빈도수 제한(빈도가 적은 단어들은 학습하지 않음)\n",
    "- workers = 학습을 위한 프로세스 수\n",
    "- sg = 0은 CBoW, 1은 Skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pending-metallic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.9233418107032776), ('skull', 0.911030113697052), ('Bang', 0.905648946762085), ('asteroid', 0.9052114486694336), ('third', 0.9020071625709534), ('baby', 0.8994219303131104), ('dog', 0.898607611656189), ('bought', 0.8975202441215515), ('rally', 0.8912495374679565), ('disc', 0.8889137506484985)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar('man')\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "innocent-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장하기\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model.wv.save_word2vec_format('~/aiffel/aiffel_projects/goingdeeper/GD5_word_embedding/w2v')\n",
    "loaded_model = KeyedVectors.load_word2vec_format('~/aiffel/aiffel_projects/goingdeeper/GD5_word_embedding/w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sensitive-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.9233418107032776), ('skull', 0.911030113697052), ('Bang', 0.905648946762085), ('asteroid', 0.9052114486694336), ('third', 0.9020071625709534), ('baby', 0.8994219303131104), ('dog', 0.898607611656189), ('bought', 0.8975202441215515), ('rally', 0.8912495374679565), ('disc', 0.8889137506484985)]\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드가 잘 되었는지 확인\n",
    "model_result = loaded_model.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-metropolitan",
   "metadata": {},
   "source": [
    "#### - Word2Vec의 OOV 문제\n",
    "\n",
    "- 사전에 없는 단어에 대해 임베딩 벡터값을 얻을 수 없다.\n",
    "- 없는 단어를 모델에 입력하면 에러가 난다.\n",
    "> KeyError: \"Key 'overacting' not present\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "toxic-encoding",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'overacting' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5175910fbd9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 에러가 나더라도 놀라지 마세요.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overacting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'overacting' not present\""
     ]
    }
   ],
   "source": [
    "# 에러가 나더라도 놀라지 마세요.\n",
    "loaded_model.most_similar('overacting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-antenna",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 임베딩 벡터의 시각화\n",
    "\n",
    "- 구글이 공개한 임베딩 벡터의 시각화 오픈소스인 임베딩 프로젝터 사용\n",
    "- 이를 통해 어떤 임베딩 벡터들이 가까운 거리에 군집되어 있고, 특정 임베딩 벡터와 유클리드 거리나 코사인 유사도가 높은지 확인할 수 있다.\n",
    "- 이를 위해서는 이미 저장된 모델로부터 벡터 값이 저장된 파일과 메타파일을 불러와야 한다!\n",
    "> $ python -m gensim.scripts.word2vec2tensor --input ~/aiffel/word_embedding/w2v --output ~/aiffel/word_embedding/w2v\n",
    "\n",
    "- 위 커맨드를 실행하면 w2v_metadata.tsv와 w2v_tensor.tsv 파일이 생성된다.\n",
    "- 해당 파일들을 다운로드한 후, [링크](https://projector.tensorflow.org/)로 이동해서 데이터를 업로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "portuguese-township",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "2021-10-06 07:29:11,451 - word2vec2tensor - INFO - running /opt/conda/lib/python3.7/site-packages/gensim/scripts/word2vec2tensor.py --input /aiffel/aiffel/aiffel_projects/goingdeeper/GD5_word_embedding/w2v --output /aiffel/aiffel/aiffel_projects/goingdeeper/GD5_word_embedding/w2v\n",
      "2021-10-06 07:29:11,451 - keyedvectors - INFO - loading projection weights from /aiffel/aiffel/aiffel_projects/goingdeeper/GD5_word_embedding/w2v\n",
      "2021-10-06 07:29:12,436 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (10363, 100) matrix of type float32 from /aiffel/aiffel/aiffel_projects/goingdeeper/GD5_word_embedding/w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2021-10-06T07:29:12.430760', 'gensim': '4.0.1', 'python': '3.7.9 | packaged by conda-forge | (default, Feb 13 2021, 20:03:11) \\n[GCC 9.3.0]', 'platform': 'Linux-5.4.0-1047-azure-x86_64-with-debian-buster-sid', 'event': 'load_word2vec_format'}\n",
      "2021-10-06 07:29:13,270 - word2vec2tensor - INFO - 2D tensor file saved to /aiffel/aiffel/aiffel_projects/goingdeeper/GD5_word_embedding/w2v_tensor.tsv\n",
      "2021-10-06 07:29:13,270 - word2vec2tensor - INFO - Tensor metadata file saved to /aiffel/aiffel/aiffel_projects/goingdeeper/GD5_word_embedding/w2v_metadata.tsv\n",
      "2021-10-06 07:29:13,270 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "!python -m gensim.scripts.word2vec2tensor --input ~/aiffel/aiffel_projects/goingdeeper/GD5_word_embedding/w2v --output ~/aiffel/aiffel_projects/goingdeeper/GD5_word_embedding/w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-example",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/80008411/136160255-a4f2e2ae-e1ca-4304-a2ab-db7548fbe8c8.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-technical",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 워드 임베딩 2: FastText\n",
    "\n",
    "- 페이스북에서 개발\n",
    "- 메커니즘 자체는 Word2Vec을 그대로 따르지만, 문자 단위 n-gram(character-level n-gram) 표현을 학습한다는 점에서 다르다. 즉 Word2Vec은 단어를 더 이상 깨질 수 없는 단위로 구분하는 반면, FastText는 단어 내부의 내부 단어(subword)들을 학습한다.\n",
    "- n-gram에서 n은 단어들이 얼마나 분리될지 결정하는 하이퍼파라미터\n",
    "- n을 3으로 잡은 tri-gram은 단어 partial을 par, art, rti, tia, ial로 분리하고 이들을 벡터로 만든다. 정확히는 시작과 끝을 <로 구분하여 <pa, art, rti, tia, ial, al>라는 6개의 subword 토큰을 벡터로 만든다.\n",
    "- 여기에 추가로 \\<partial> 자체도 벡터화한다.\n",
    "- 실제로는 n을 최소값, 최대값으로 범위를 설정할 수 있는데, gensim 패키지에서는 기본값으로 각각 3, 6이 설정되어 있다. 이 경우 다음과 같은 subword가 벡터화된다.\n",
    "> <pa, art, rti, ita, ial, al>, <par, arti, rtia, tial, ial>, 중략, \\<partial>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-threat",
   "metadata": {},
   "source": [
    "#### - 학습 방법\n",
    "\n",
    "- 마찬가지로 네거티브 샘플링을 사용한다. 즉 (중심 단어, 주변 단어)의 쌍을 가지고 이 쌍이 positive인지, negative인지 예측을 진행하는 것.\n",
    "- 다만 Word2Vec과의 차이는 학습 과정에서 중심 단어에 속한 문자 단위 n-gram 단어 벡터들을 모두 업데이트한다는 점이다.\n",
    "\n",
    "#### - OOV와 오타에 대한 대응\n",
    "\n",
    "- Word2Vec과는 달리 OOV와 오타에 robust하다. 즉 단어장에 없는 단어라도, 해당 단어의 n-gram이 다른 단어에 존재하면 이로부터 벡터값을 얻는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "moving-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "fasttext_model = FastText(corpus, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "modern-ebony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('resolving', 0.9394947290420532),\n",
       " ('fluctuating', 0.938098132610321),\n",
       " ('malting', 0.9359013438224792),\n",
       " ('mounting', 0.9339149594306946),\n",
       " ('emptying', 0.9323922991752625),\n",
       " ('extracting', 0.9315359592437744),\n",
       " ('shooting', 0.9312771558761597),\n",
       " ('debilitating', 0.929542601108551),\n",
       " ('overwhelming', 0.9291026592254639),\n",
       " ('lifting', 0.9286652207374573)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec에서 에러가 발생했던 단어들을 FastText 모델에 넣어보자\n",
    "fasttext_model.wv.most_similar('overacting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "subjective-rebound",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('memory', 0.9469051361083984),\n",
       " ('musical', 0.8639096021652222),\n",
       " ('mechanisms', 0.8625136613845825),\n",
       " ('mechanism', 0.8617560267448425),\n",
       " ('basic', 0.8538733124732971),\n",
       " ('imagine', 0.8524118661880493),\n",
       " ('mechanical', 0.8492914438247681),\n",
       " ('technical', 0.8422043323516846),\n",
       " ('intelligence', 0.8375174403190613),\n",
       " ('specific', 0.8353775143623352)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar('memoryy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-clearing",
   "metadata": {},
   "source": [
    "#### - 한국어에서의 FastText\n",
    "\n",
    "- 한국어도 FastText 방식으로 학습이 가능하다. n-gram이 음절 또는 자소 단위라고 보는 것이다.\n",
    "- 1) 음절 단위\n",
    "    - n=3일 때\n",
    "    > 텐서플로우 = <텐서, 텐서플, 서플로, 플로우, 로우>, <텐서플로우>\n",
    "- 2) 자소 단위\n",
    "    - 단어의 초성, 중성, 종성을 분리한다고 하고, 종성이 존재하지 않을 경우에는 \\_토큰을 대신 사용한다.\n",
    "    > 텐서플로우 = <ㅌㅔ, ㅌㅔㄴ, ㅔㄴㅅ, ㄴㅅㅓ, ㅅㅓ_, 중략 >\n",
    "    - 이 경우 고유명사의 의미적 특성을 학습하는 데는 크게 도움이 되지 않거나 오히려 성능이 떨어진다. 어휘를 분해하여 이해할 수 있는 복합명사는 의미적 특징을 효과적으로 잡아낼 수 있지만, 고유명사는 어휘를 분해해도 아무 이득을 볼 수 없기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-richmond",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 워드 임베딩 3: GloVe\n",
    "\n",
    "- Global Vectors for Word Representation은 2014년 미국 스탠포드 대학에서 개발한 워드 임베딩 방법로닝다.\n",
    "- 가장 큰 특징은 카운트 기반과 예측 기반의 방법을 모두 사용했다는 것이다.\n",
    "\n",
    "### 잠재 의미 분석(LSA, Latent Semantic Analysis)\n",
    "\n",
    "- 단어의 빈도를 수치화한 DTM을 차원 축소하여 밀집 표현으로 임베딩하는 방법\n",
    "- 즉 DTM에 특잇값 분해를 사용해 잠재된 의미를 이끌어내는 방법론\n",
    "- 결국 단어를 카운트해서 만든 DTM을 입력으로 하므로 카운트 기반의 임베딩 방법이라고 볼 수 있는데, 이는 몇 가지 한계를 가진다.\n",
    "    - 1) 차원 축소의 특성으로 인해 새로운 단어가 추가되면 다시 DTM을 만들어 새로 차원 축소를 해야 한다.\n",
    "    - 2) 단어 벡터간 유사도를 계산하는 측면에서 Word2Vec보다 성능이 떨어진다.\n",
    "\n",
    "- LSA와 대조되는 예측 기반 방법은 Word2Vec과 같은 방법을 말한다. Word2Vec은 LSA보다 단어 벡터 간 유사도를 구하는 능력은 뛰어나지만, 코퍼스의 전체적인 통계 정보를 활용하지 못한다는 한계가 있다.\n",
    "- 따라서 두 가지를 모두 사용하는 GloVe가 등장하게 된다.\n",
    "\n",
    "### 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix)\n",
    "\n",
    "- GloVe를 이해하기 위해서는 윈도우 기반 동시 등장 행렬의 정의를 이해해야 한다.\n",
    "- 예를 들어 다음 문장의 코퍼스가 있다고 해보자.\n",
    "    - I like deep learning.\n",
    "    - I like NLP.\n",
    "    - I enjoy flying.\n",
    "- 이로부터 만들어진 동시 등장 행렬은 다음과 같다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136167412-0e3d9bcf-b822-4cdb-a7ae-48e5421d4109.png)\n",
    "\n",
    "- 행과 열을 전체 단어장의 단어들로 구성하고, 어떤 i 단어의 윈도우 크기 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-mileage",
   "metadata": {},
   "source": [
    "### 동시 등장 확률(Co-occurrence Probability)\n",
    "\n",
    "- $P(k|i)$ 동시 등장 확률은 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률이다.\n",
    "- i를 중심 단어, k를 주변 단어라고 한다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136167879-89754384-b358-4c79-9c1d-0118e4a48a79.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-utility",
   "metadata": {},
   "source": [
    "### 손실 함수\n",
    "\n",
    "- GloVe는 동시 등장 행렬로부터 계산된 동시 등장 확률을 이용해 손실 함수를 설계한다.\n",
    "- GloVe의 아이디어를 한 줄로 요약하면, 중심 단어 벡터와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 빈도의 로그값이 되도록 만드는 것.\n",
    "- 즉 전체 코퍼스에서의 동시 등장 빈도의 로그값과 중심 단어 벡터와 주변 단어 벡터의 내적값의 차이가 최소화되도록 두 벡터의 값을 학습하는 것\n",
    "\n",
    "- GloVe의 손실 함수 식과 변수들\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136169006-bceb1aa6-9ca5-4c6a-b76b-8030dc031115.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136168911-cdd0c174-f68e-4b09-9922-c815ffc48e8e.png)\n",
    "\n",
    "- $f(X_{ik})$는 동시 등장 빈도의 값으로 굉장히 낮을 경우에는 거의 도움이 되지 않는 정보이다. 그래서 여기에 가중치를 주기 위해 가중치 함수를 도입했다.\n",
    "- 아래와 같이 $f(X_{ik})$ 값이 작으면 함수 값은 작아지고, 값이 크면 함수 값도 커진다. 하지만 큰 값에 대해서 지나친 가중치를 주지 않기 위해 최대값을 1로 정했다. 이는 'it is'와 같은 불용어의 동시 등장 빈도수가 높을 때 지나친 가중을 주지 않기 위함이다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/80008411/136169043-c1b7fb14-3fe9-424a-963d-d72b3b6ffa19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-conflict",
   "metadata": {},
   "source": [
    "### GloVe 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "established-lease",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /aiffel/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /aiffel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "guilty-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "corpus = movie_reviews.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "coastal-hearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 20 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n"
     ]
    }
   ],
   "source": [
    "from glove import Corpus, Glove\n",
    "\n",
    "# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n",
    "emb = Corpus()\n",
    "emb.fit(corpus, window=5)\n",
    "\n",
    "# 벡터의 차원은 100, 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "glove.fit(emb.matrix, epochs=20, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(emb.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "burning-banking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result1 [('woman', 0.9541682319723492), ('young', 0.8944593575706618), ('girl', 0.8887817885942938), ('boy', 0.8826109016093757)]\n",
      "model_result2 [('science', 0.9826217148867289), ('pulp', 0.9679211348516671), ('kong', 0.6976340973632392), ('hong', 0.6809053655622014)]\n"
     ]
    }
   ],
   "source": [
    "model_result1 = glove.most_similar(\"man\")\n",
    "model_result2 = glove.most_similar(\"fiction\")\n",
    "\n",
    "print(\"model_result1\", model_result1)\n",
    "print(\"model_result2\", model_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-composer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
